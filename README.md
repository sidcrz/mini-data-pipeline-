# ðŸš€ Mini Data Pipeline â€” Python + PostgreSQL + Docker (Deployed on Google Cloud)

This project marks my first cloud-based ETL workflow, built step by step inside a Google Cloud VM using Docker, PostgreSQL, and Python (pandas + SQLAlchemy).

---

## ðŸ§  Architecture Diagram

Hereâ€™s how the entire mini data pipeline fits together:

```

+---------------------------+
|      Python Script        |
|  (pandas + SQLAlchemy)    |
|  Extract + Transform Data |
+------------+--------------+
             |
             v
+---------------------------+
|   PostgreSQL (Docker)     |
|   demo database container |
+------------+--------------+
             |
             v
+---------------------------+
|   Query + Verify Results  |
+---------------------------+
             |
             v
+---------------------------+
| Google Cloud VM (Ubuntu)  |
|  Environment where all    |
|  components are deployed  |
+---------------------------+

```
---

## ðŸ’¬ How It All Works (In Simple Terms)

This setup is like building a small data factory.

Raw data comes in, gets cleaned and organized, and is finally stored neatly so analysts or dashboards can use it.  
Hereâ€™s how each part fits in:

- ðŸ³ Docker acts like a safe box that runs your database in isolation â€” no messy installations or version conflicts.
- ðŸ˜ PostgreSQL is your warehouse where all the transformed data is stored.
- ðŸ Python is the worker who moves the data, cleans it, and loads it into the warehouse.
- â˜ï¸ Google Cloud VM is your remote workspace, similar to a mini production server where everything runs together.

Itâ€™s simple, reproducible, and mirrors the foundation of how modern data pipelines work.

---

## ðŸ§­ Why This Project Matters

Every modern company â€” Netflix, Uber, Spotify, Airbnb â€” depends on a process like this every single day.  
Their data engineers build pipelines that collect raw data, clean it, and prepare it for analytics or machine learning.

This project helped me:
- Understand the core ETL (Extract, Transform, Load) process  
- Learn how Docker keeps services consistent and portable  
- Use Pythonâ€™s pandas and SQLAlchemy to automate data flows  
- Simulate a small production-like environment on Google Cloud  

Even though this project is small, it captures the real mindset of a Data Engineer â€” building reliable, repeatable systems that move data efficiently.

---

## âš¡ï¸ Under the Hood (What Really Happens)

1. A PostgreSQL container is launched using Docker and configured to persist data inside a local volume.  
2. A Python ETL script extracts mock data, performs transformations with pandas, and loads it into PostgreSQL using SQLAlchemy.  
3. Everything runs seamlessly inside a Google Cloud VM â€” imitating a true production setup.  
4. I connect directly to the PostgreSQL database to verify that the transformed data is successfully loaded.

This workflow covers the exact building blocks that scale later into complex, automated data systems.

---

## ðŸŒ Real-World Analogy

Think of this setup like cooking a meal:

Component | Real-World Role
-----------|----------------
ðŸ Python | The chef who prepares and cleans the ingredients (raw data)
ðŸ˜ PostgreSQL | The kitchen counter where the finished dishes (clean data) are served
ðŸ³ Docker | The kitchen setup that can be moved anywhere without breaking anything
â˜ï¸ Google Cloud VM | The restaurant space where your kitchen runs and customers (analysts, dashboards) are served

Once you understand this small pipeline, you can scale it up easily â€” adding Airflow for scheduling, dbt for transformations, or Spark for big data processing.


## ðŸ§© 2. Run PostgreSQL in a Docker Container

With the environment ready, I set up PostgreSQL inside Docker.  
This keeps everything portable and avoids local installation issues.

```
docker run -d \
  --name postgres \
  -e POSTGRES_PASSWORD=admin \
  -e POSTGRES_DB=demo \
  -p 5432:5432 \
  -v pgdata:/var/lib/postgresql/data \
  postgres 
  ```

Explanation:
```
- -d runs the container in the background  
- --name postgres gives it a friendly name  
- -e POSTGRES_PASSWORD and -e POSTGRES_DB set environment variables  
- -p 5432:5432 exposes the database to the host  
- -v pgdata ensures persistent storage

```

Verify itâ€™s running:

docker ps

Expected:
CONTAINER ID   IMAGE      STATUS          PORTS                    NAMES
xxxxxx         postgres   Up ...          0.0.0.0:5432->5432/tcp   postgres

---

## ðŸ§© 3. Set Up the Python Environment

Next, I created a Python virtual environment for this project.  
Itâ€™s good practice to keep dependencies isolated and organized.

sudo apt install python3-venv -y
python3 -m venv venv
source venv/bin/activate

Install dependencies:
pip install pandas sqlalchemy psycopg2-binary

Library | Role
---------|-----
pandas | Data cleaning and transformation
SQLAlchemy | Database connection and ORM
psycopg2-binary | PostgreSQL driver

---

## ðŸ§© 4. Build the ETL Script

Hereâ€™s the full ETL script that extracts mock data, transforms it, and loads it into PostgreSQL.

etl_script.py

```
import pandas as pd
from sqlalchemy import create_engine 
```

# Step 1: Extract
```
data = {
    'id': [1, 2, 3],
    'name': ['Alice', 'Bob', 'Clara'],
    'country': ['USA', 'India', 'UK']
}
df = pd.DataFrame(data)
print("âœ… Data extracted successfully")
```

# Step 2: Transform
```
df['name'] = df['name'].str.upper()
print("ðŸ”„ Data transformed successfully")
```

# Step 3: Load
```
engine = create_engine('postgresql://postgres:admin@localhost:5432/demo')
df.to_sql('customers', engine, if_exists='replace', index=False)
print("ðŸ“¦ Data loaded into PostgreSQL successfully!")
```

Run it:
```
python3 etl_script.py
```

Expected output:
âœ… Data extracted successfully
ðŸ”„ Data transformed successfully
ðŸ“¦ Data loaded into PostgreSQL successfully!

---

## ðŸ§© 5. Verify the Data in PostgreSQL

Finally, I connected to PostgreSQL to make sure the ETL worked.

```
docker exec -it postgres psql -U postgres -d demo
```

Inside the psql prompt:

SELECT * FROM customers;
```
Output:
 id |  name  | country
----+--------+---------
  1 | ALICE  | USA
  2 | BOB    | India
  3 | CLARA  | UK
(3 rows)
```
âœ… The ETL worked perfectly â€” data extracted, cleaned, and stored successfully.

---

## ðŸ§© 6. Organize the Project
```
mini-data-pipeline/
â”‚
â”œâ”€â”€ etl_script.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile (optional)
â”œâ”€â”€ README.md
â””â”€â”€ screenshots/
    â”œâ”€â”€ 01_vm_created.png
    â”œâ”€â”€ 02_vm_ssh.png
    â”œâ”€â”€ 03_docker_ready.png
    â”œâ”€â”€ 04_postgres_pull.png
    â”œâ”€â”€ 05_postgres_running.png
    â”œâ”€â”€ 06_venv_active.png
    â”œâ”€â”€ 07_pip_installed.png
    â”œâ”€â”€ 08_etl_success.png
    â””â”€â”€ 09_sql_output.png

```
This organization keeps the project clean, reproducible, and ready for version control.

---

## ðŸŽ“ What I Learned

- How to use Docker for persistent, portable databases  
- How to connect Python â†’ PostgreSQL using SQLAlchemy  
- The full ETL lifecycle (extract, transform, load)  
- How to run pipelines in a cloud-based environment  

These are the exact building blocks for every modern data stack.

---

