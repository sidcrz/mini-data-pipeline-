# ğŸš€ Mini Data Pipeline â€” Python + PostgreSQL + Docker (Deployed on Google Cloud)

This project marks my first cloud-based ETL workflow, built step by step inside a Google Cloud VM using Docker, PostgreSQL, and Python (pandas + SQLAlchemy).

---

## ğŸ§  Architecture Diagram

Hereâ€™s how the entire mini data pipeline fits together:

```

+---------------------------+
|      Python Script        |
|  (pandas + SQLAlchemy)    |
|  Extract + Transform Data |
+------------+--------------+
             |
             v
+---------------------------+
|   PostgreSQL (Docker)     |
|   demo database container |
+------------+--------------+
             |
             v
+---------------------------+
|   Query + Verify Results  |
+---------------------------+
             |
             v
+---------------------------+
| Google Cloud VM (Ubuntu)  |
|  Environment where all    |
|  components are deployed  |
+---------------------------+

```
---

## ğŸ’¬ How It All Works (In Simple Terms)

This setup is like building a small data factory.

Raw data comes in, gets cleaned and organized, and is finally stored neatly so analysts or dashboards can use it.  
Hereâ€™s how each part fits in:

- ğŸ³ Docker acts like a safe box that runs your database in isolation â€” no messy installations or version conflicts.
- ğŸ˜ PostgreSQL is your warehouse where all the transformed data is stored.
- ğŸ Python is the worker who moves the data, cleans it, and loads it into the warehouse.
- â˜ï¸ Google Cloud VM is your remote workspace, similar to a mini production server where everything runs together.

Itâ€™s simple, reproducible, and mirrors the foundation of how modern data pipelines work.

---

## ğŸ§­ Why This Project Matters

Every modern company â€” Netflix, Uber, Spotify, Airbnb â€” depends on a process like this every single day.  
Their data engineers build pipelines that collect raw data, clean it, and prepare it for analytics or machine learning.

This project helped me:
- Understand the core ETL (Extract, Transform, Load) process  
- Learn how Docker keeps services consistent and portable  
- Use Pythonâ€™s pandas and SQLAlchemy to automate data flows  
- Simulate a small production-like environment on Google Cloud  

Even though this project is small, it captures the real mindset of a Data Engineer â€” building reliable, repeatable systems that move data efficiently.

---

## âš¡ï¸ Under the Hood (What Really Happens)

1. A PostgreSQL container is launched using Docker and configured to persist data inside a local volume.  
2. A Python ETL script extracts mock data, performs transformations with pandas, and loads it into PostgreSQL using SQLAlchemy.  
3. Everything runs seamlessly inside a Google Cloud VM â€” imitating a true production setup.  
4. I connect directly to the PostgreSQL database to verify that the transformed data is successfully loaded.

This workflow covers the exact building blocks that scale later into complex, automated data systems.

---

## ğŸŒ Real-World Analogy

Think of this setup like cooking a meal:

Component | Real-World Role
-----------|----------------
ğŸ Python | The chef who prepares and cleans the ingredients (raw data)
ğŸ˜ PostgreSQL | The kitchen counter where the finished dishes (clean data) are served
ğŸ³ Docker | The kitchen setup that can be moved anywhere without breaking anything
â˜ï¸ Google Cloud VM | The restaurant space where your kitchen runs and customers (analysts, dashboards) are served

Once you understand this small pipeline, you can scale it up easily â€” adding Airflow for scheduling, dbt for transformations, or Spark for big data processing.

 <img width="972" height="276" alt="Screenshot 2025-10-22 at 12 10 08â€¯AM" src="https://github.com/user-attachments/assets/d8cded58-f227-40e9-9a3c-8c69d33f6462" />

## ğŸ§© 2. Run PostgreSQL in a Docker Container

With the environment ready, I set up PostgreSQL inside Docker.  
This keeps everything portable and avoids local installation issues.

```
docker run -d \
  --name postgres \
  -e POSTGRES_PASSWORD=admin \
  -e POSTGRES_DB=demo \
  -p 5432:5432 \
  -v pgdata:/var/lib/postgresql/data \
  postgres 
  ```
 




Explanation:
```
- -d runs the container in the background  
- --name postgres gives it a friendly name  
- -e POSTGRES_PASSWORD and -e POSTGRES_DB set environment variables  
- -p 5432:5432 exposes the database to the host  
- -v pgdata ensures persistent storage

```

Verify itâ€™s running:

docker ps

Expected:
CONTAINER ID   IMAGE      STATUS          PORTS                    NAMES
xxxxxx         postgres   Up ...          0.0.0.0:5432->5432/tcp   postgres

---

## ğŸ§© 3. Set Up the Python Environment

Next, I created a Python virtual environment for this project.  
Itâ€™s good practice to keep dependencies isolated and organized.

<img width="1440" height="594" alt="Screenshot 2025-10-22 at 12 13 49â€¯AM" src="https://github.com/user-attachments/assets/1b7c3ec1-f702-4951-ac9c-927f09c0d43f" />

sudo apt install python3-venv -y
python3 -m venv venv
source venv/bin/activate

Install dependencies:
pip install pandas sqlalchemy psycopg2-binary

Library | Role
---------|-----
pandas | Data cleaning and transformation
SQLAlchemy | Database connection and ORM
psycopg2-binary | PostgreSQL driver

---

## ğŸ§© 4. Build the ETL Script

Hereâ€™s the full ETL script that extracts mock data, transforms it, and loads it into PostgreSQL.

etl_script.py

```
import pandas as pd
from sqlalchemy import create_engine 
```

# Step 1: Extract
```
data = {
    'id': [1, 2, 3],
    'name': ['Alice', 'Bob', 'Clara'],
    'country': ['USA', 'India', 'UK']
}
df = pd.DataFrame(data)
print("âœ… Data extracted successfully")
```

# Step 2: Transform
```
df['name'] = df['name'].str.upper()
print("ğŸ”„ Data transformed successfully")
```

# Step 3: Load
```
engine = create_engine('postgresql://postgres:admin@localhost:5432/demo')
df.to_sql('customers', engine, if_exists='replace', index=False)
print("ğŸ“¦ Data loaded into PostgreSQL successfully!")
```

Run it:
```
python3 etl_script.py
```

Expected output:
âœ… Data extracted successfully
ğŸ”„ Data transformed successfully
ğŸ“¦ Data loaded into PostgreSQL successfully!

---

## ğŸ§© 5. Verify the Data in PostgreSQL

Finally, I connected to PostgreSQL to make sure the ETL worked.

```
docker exec -it postgres psql -U postgres -d demo
```

Inside the psql prompt:

SELECT * FROM customers;
```
Output:
 id |  name  | country
----+--------+---------
  1 | ALICE  | USA
  2 | BOB    | India
  3 | CLARA  | UK
(3 rows)
```
<img width="807" height="293" alt="Screenshot 2025-10-22 at 12 12 43â€¯AM" src="https://github.com/user-attachments/assets/cbd9bfe7-685d-4692-bd96-2e382132a4a2" />


âœ… The ETL worked perfectly â€” data extracted, cleaned, and stored successfully.

---

## ğŸ§© 6. Organize the Project
```
mini-data-pipeline/
â”‚
â”œâ”€â”€ etl_script.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile (optional)
â”œâ”€â”€ README.md
```
This organization keeps the project clean, reproducible, and ready for version control.

---

## ğŸ“ What I Learned

- How to use Docker for persistent, portable databases  
- How to connect Python â†’ PostgreSQL using SQLAlchemy  
- The full ETL lifecycle (extract, transform, load)  
- How to run pipelines in a cloud-based environment  

These are the exact building blocks for every modern data stack.

---

